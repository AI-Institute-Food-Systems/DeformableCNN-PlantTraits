{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/plant-ai-biophysics-lab/DeformableCNN-PlantTraits/blob/main/example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluation Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data and config setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch, torchvision\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.functional import split\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import lr_scheduler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from datatools import *\n",
    "from engine import train_single_epoch, validate\n",
    "from loss import NMSELoss\n",
    "from architecture import GreenhouseMidFusionRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download 2021 Autonomous Greenhouse Challenge dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  844M    0  844M    0     0  4203k      0 --:--:--  0:03:25 --:--:-- 3329k\n",
      "curl: (6) Could not resolve host: data.zip\n",
      "unzip:  cannot find or open data.zip, data.zip.zip or data.zip.ZIP.\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "!mkdir data\n",
    "!wget https://data.4tu.nl/ndownloader/files/28906503 -O data/data.zip #for linux OS\n",
    "# !curl https://data.4tu.nl/ndownloader/files/28906503 -O data.zip #for macOS\n",
    "!unzip data/data.zip\n",
    "!rm data/data.zip\n",
    "\n",
    "clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define data and output directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sav_dir='model_output/'\n",
    "if not os.path.exists(sav_dir):\n",
    "    os.mkdir(sav_dir)\n",
    "RGB_Data_Dir   = '/data/RGBImages/'\n",
    "Depth_Data_Dir = '/data/DepthImages/'  \n",
    "JSON_Files_Dir = '/data/GroundTruth_All_388_Images.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set model architectures options:\n",
    "- single vs. multi input (SI- or MI-)\n",
    "- single vs. multi output (-SO or -MO)\n",
    "- deformable vs. standard convolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConvType = 'deformable' # 'standard'\n",
    "\n",
    "training_category = 'MIMO' #'MIMO', 'MISO', 'SIMO', 'SISO'\n",
    "\n",
    "# Multi-input, multi-output model\n",
    "if training_category   == 'MIMO':\n",
    "    transform_type = get_transforms(train=False) \n",
    "    inputs = ['RGB-D']\n",
    "    outputs = ['All']\n",
    "    NumOutputs = 5\n",
    "    \n",
    "# Multi-input, single-output model\n",
    "elif training_category == 'MISO':\n",
    "    transform_type = get_transforms(train=False)\n",
    "    inputs = ['RGB-D']\n",
    "    outputs = ['FW','DW','H','D','LA']\n",
    "    NumOutputs = 1\n",
    "    \n",
    "# Single-input, multi-output model\n",
    "elif training_category == 'SIMO':\n",
    "    transform_type = get_RGB_transforms(train=False)\n",
    "    inputs = ['RGB','D']\n",
    "    outputs = ['All']\n",
    "    NumOutputs = 5\n",
    "    \n",
    "# Single-input, single-output model\n",
    "elif training_category == 'SISO':\n",
    "    transform_type = get_RGB_transforms(train=False)\n",
    "    inputs = ['RGB','D']\n",
    "    outputs = ['FW','DW','H','D','LA']\n",
    "    NumOutputs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set other model config parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_seed = 12    \n",
    "num_epochs = 400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create PyTorch dataset, create PyTorch dataloader, and split train/val/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the PyTorch datalaoder the autonomous greenhouse dataset.\n",
    "dataset = GreenhouseDataset(rgb_dir = RGB_Data_Dir, \n",
    "                            d_dir = Depth_Data_Dir, \n",
    "                            jsonfile_dir = JSON_Files_Dir, \n",
    "                            transforms = transform_type) \n",
    "\n",
    "# Remove last 50 images from training/validation set. These are the test set.                         \n",
    "dataset.df= dataset.df.iloc[:-50]\n",
    "\n",
    "# Split train and validation set. Stratify based on variety.\n",
    "train_split, val_split = train_test_split(dataset.df, \n",
    "                                          test_size = 0.2, \n",
    "                                          random_state = split_seed, \n",
    "                                          stratify = dataset.df['Variety'])\n",
    "train = torch.utils.data.Subset(dataset, train_split.index.tolist())\n",
    "val   = torch.utils.data.Subset(dataset, val_split.index.tolist())\n",
    "dataset.set_indices(train.indices, val.indices)\n",
    "                                                                                     \n",
    "# Create train and validation dataloaders\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=10, num_workers=12, shuffle=True)#, sampler=train_sampler)\n",
    "val_loader   = torch.utils.data.DataLoader(val,   batch_size=10, shuffle=False, num_workers=12)#, sampler=val_sampler)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the loss function as Normalized Mean Squared Error, as required for the 2021 Autonomous Greenhouse Challenge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = NMSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the training loop and fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "device = torch.device('cuda')\n",
    "\n",
    "for input in inputs:\n",
    "    for output in outputs:\n",
    "        dataset.input = input\n",
    "        dataset.out = output\n",
    "        model = GreenhouseMidFusionRegressor(input_data_type = input, num_outputs = NumOutputs, conv_type = ConvType)\n",
    "        model.to(device)\n",
    "        params = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "        optimizer = torch.optim.Adam(params, \n",
    "                                     lr=0.0005, \n",
    "                                     betas=(0.9, 0.999), \n",
    "                                     eps=1e-08, \n",
    "                                     weight_decay = 0, \n",
    "                                     amsgrad = False)  # select an optimzer for each run\n",
    "    \n",
    "                                \n",
    "        best_val_loss = 9999999 # initial dummy value\n",
    "        current_val_loss = 0\n",
    "        # training_val_loss=0\n",
    "           \n",
    "        writer = SummaryWriter()\n",
    "        start = time.time()\n",
    "                                \n",
    "        for epoch in range(num_epochs):\n",
    "            with open('run.txt', 'a') as f:\n",
    "                f.write('\\n')\n",
    "                f.write('Epoch: '+ str(epoch + 1) + ', Time Elapsed: '+ str((time.time()-start)/60) + ' mins')\n",
    "            print('Epoch: ', str(epoch + 1), ', Time Elapsed: ', str((time.time()-start)/60), ' mins')\n",
    "\n",
    "            train_single_epoch(model, dataset, device, criterion, optimizer, writer, epoch, train_loader)\n",
    "\n",
    "            best_val_loss = validate(model, dataset, device, training_category, sav_dir, criterion, writer, epoch, val_loader, best_val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evalutation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the PyTorch datalaoder the autonomous greenhouse dataset.\n",
    "testset = GreenhouseDataset(rgb_dir = RGB_Data_Dir, \n",
    "                            d_dir = Depth_Data_Dir, \n",
    "                            jsonfile_dir = JSON_Files_Dir, \n",
    "                            transforms = transform_type)\n",
    "\n",
    "# Grab last 50 images as test dataset\n",
    "testset.df = testset.df[-50:]\n",
    "\n",
    "# Get testset_size\n",
    "testset_size = testset.df.shape[0]\n",
    "\n",
    "# Create test dataloader\n",
    "test_loader = torch.utils.data.DataLoader(testset, \n",
    "                                          batch_size = 50,\n",
    "                                          num_workers = 0, \n",
    "                                          shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define loss functions for model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cri = NMSELoss()\n",
    "mse = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the evaluation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input is  RGB-D\n",
      "FW MSE:  16857.876953125\n",
      "DW MSE:  4.854626655578613\n",
      "H MSE:  3.97654390335083\n",
      "D MSE:  22.738414764404297\n",
      "LA MSE:  5795591.0\n",
      "Overall NMSE:  1.632205843925476\n"
     ]
    }
   ],
   "source": [
    "# Evaluation loop\n",
    "device=torch.device('cuda')\n",
    "\n",
    "with torch.no_grad():\n",
    "    for input in inputs:\n",
    "        final = torch.zeros((testset_size,0))\n",
    "        all_targets = torch.zeros((testset_size,0))\n",
    "        for output in outputs:\n",
    "            print('Input is ', input)\n",
    "            testset.input = input\n",
    "            testset.out = output\n",
    "\n",
    "            device=torch.device('cuda')\n",
    "            model= GreenhouseMidFusionRegressor(input_data_type = input, \n",
    "                                                num_outputs = NumOutputs, \n",
    "                                                conv_type = ConvType)\n",
    "            model.to(device)\n",
    "            model.load_state_dict(torch.load(sav_dir + 'bestmodel' + training_category + '_' + input + '_' + output + '.pth'))\n",
    "            model.eval()\n",
    "\n",
    "\n",
    "            if output=='All':\n",
    "                ap=torch.zeros((0,5))\n",
    "                at=torch.zeros((0,5))\n",
    "            else:\n",
    "                ap=torch.zeros((0,1))\n",
    "                at=torch.zeros((0,1))\n",
    "\n",
    "            for rgbd, targets in test_loader:\n",
    "                rgbd = rgbd.to(device)\n",
    "                targets = targets.to(device)\n",
    "                preds = model(rgbd)\n",
    "                # mse_loss=mse(preds, targets)\n",
    "                # nmse=criterion(preds, targets)\n",
    "                # nmse, pred=cri(preds, targets)\n",
    "                ap=torch.cat((ap, preds.detach().cpu()), 0)\n",
    "                at=torch.cat((at, targets.detach().cpu()), 0)\n",
    "\n",
    "            if output=='All':\n",
    "                print('FW MSE: ', str(mse(ap[:,0],at[:,0]).tolist()))\n",
    "                print('DW MSE: ', str(mse(ap[:,1],at[:,1]).tolist()))\n",
    "                print('H MSE: ', str(mse(ap[:,2],at[:,2]).tolist()))\n",
    "                print('D MSE: ', str(mse(ap[:,3],at[:,3]).tolist()))\n",
    "                print('LA MSE: ', str(mse(ap[:,4],at[:,4]).tolist()))\n",
    "            else:\n",
    "                final=torch.cat((final, ap.detach().cpu()),1)\n",
    "                all_targets=torch.cat((all_targets, at.detach().cpu()),1)\n",
    "                print(output,' MSE: ', str(mse(ap,at).tolist()))\n",
    "\n",
    "        if output == 'All':\n",
    "            print('Overall NMSE: ', str(cri(ap,at).tolist()))\n",
    "        else:\n",
    "            print('Overall NMSE: ', str(cri(final,all_targets).tolist()))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "52f36c1f6eb8678621dd418d5b8ad0837811436cfff124e7936f8a687fb60368"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('greenhouse': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
